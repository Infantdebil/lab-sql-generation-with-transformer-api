{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Infantdebil/lab-sql-generation-with-transformer-api/blob/main/lab-sql-generation-with-transformer-api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07J-g-SenuuU"
      },
      "source": [
        "# SQL Generation with Transformer API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bAMjQKJfG3d"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers bitsandbytes accelerate sqlparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzllQomZfQnj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hztT0MXkfRs1"
      },
      "outputs": [],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jsl838clW_f"
      },
      "outputs": [],
      "source": [
        "available_memory = torch.cuda.get_device_properties(0).total_memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTQnCfsen16q"
      },
      "outputs": [],
      "source": [
        "print(available_memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt4ilTkpTMoL"
      },
      "source": [
        "##Download the Model\n",
        "Use any model on Colab (or any system with >30GB VRAM on your own machine) to load this in f16. If unavailable, use a GPU with minimum 8GB VRAM to load this in 8bit, or with minimum 5GB of VRAM to load in 4bit.\n",
        "\n",
        "This step can take around 5 minutes the first time. So please be patient :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mNT25UOfSMw"
      },
      "outputs": [],
      "source": [
        "model_name = \"defog/sqlcoder-7b-2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if available_memory > 15e9:\n",
        "    # if you have atleast 15GB of GPU memory, run load the model in float16\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        use_cache=True,\n",
        "    )\n",
        "else:\n",
        "    # else, load in 8 bits – this is a bit slower\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        trust_remote_code=True,\n",
        "        # torch_dtype=torch.float16,\n",
        "        load_in_8bit=True,\n",
        "        device_map=\"auto\",\n",
        "        use_cache=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzLcpXyzTkHM"
      },
      "source": [
        "##Set the Question & Prompt and Tokenize\n",
        "Feel free to change the schema in the prompt below to your own schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eI-VpCkf-fN"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"### Task\n",
        "Generate a SQL query to answer [QUESTION]{question}[/QUESTION]\n",
        "\n",
        "### Instructions\n",
        "- If you cannot answer the question with the available database schema, return 'I do not know'\n",
        "- Remember that revenue is price multiplied by quantity\n",
        "- Remember that cost is supply_price multiplied by quantity\n",
        "\n",
        "### Database Schema\n",
        "This query will run on a database whose schema is represented in this string:\n",
        "CREATE TABLE products (\n",
        "  product_id INTEGER PRIMARY KEY, -- Unique ID for each product\n",
        "  name VARCHAR(50), -- Name of the product\n",
        "  price DECIMAL(10,2), -- Price of each unit of the product\n",
        "  quantity INTEGER  -- Current quantity in stock\n",
        ");\n",
        "\n",
        "CREATE TABLE customers (\n",
        "   customer_id INTEGER PRIMARY KEY, -- Unique ID for each customer\n",
        "   name VARCHAR(50), -- Name of the customer\n",
        "   address VARCHAR(100) -- Mailing address of the customer\n",
        ");\n",
        "\n",
        "CREATE TABLE salespeople (\n",
        "  salesperson_id INTEGER PRIMARY KEY, -- Unique ID for each salesperson\n",
        "  name VARCHAR(50), -- Name of the salesperson\n",
        "  region VARCHAR(50) -- Geographic sales region\n",
        ");\n",
        "\n",
        "CREATE TABLE sales (\n",
        "  sale_id INTEGER PRIMARY KEY, -- Unique ID for each sale\n",
        "  product_id INTEGER, -- ID of product sold\n",
        "  customer_id INTEGER,  -- ID of customer who made purchase\n",
        "  salesperson_id INTEGER, -- ID of salesperson who made the sale\n",
        "  sale_date DATE, -- Date the sale occurred\n",
        "  quantity INTEGER -- Quantity of product sold\n",
        ");\n",
        "\n",
        "CREATE TABLE product_suppliers (\n",
        "  supplier_id INTEGER PRIMARY KEY, -- Unique ID for each supplier\n",
        "  product_id INTEGER, -- Product ID supplied\n",
        "  supply_price DECIMAL(10,2) -- Unit price charged by supplier\n",
        ");\n",
        "\n",
        "-- sales.product_id can be joined with products.product_id\n",
        "-- sales.customer_id can be joined with customers.customer_id\n",
        "-- sales.salesperson_id can be joined with salespeople.salesperson_id\n",
        "-- product_suppliers.product_id can be joined with products.product_id\n",
        "\n",
        "### Answer\n",
        "Given the database schema, here is the SQL query that answers [QUESTION]{question}[/QUESTION]\n",
        "[SQL]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2Lyi9VcVoTA"
      },
      "source": [
        "##Generate the SQL\n",
        "This can be excruciatingly slow on a T4 in Colab, and can take 10-20 seconds per query. On faster GPUs, this will take ~1-2 seconds\n",
        "\n",
        "Ideally, you should use `num_beams`=4 for best results. But because of memory constraints, we will stick to just 1 for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iBKg6Rmmpnw"
      },
      "outputs": [],
      "source": [
        "import sqlparse\n",
        "\n",
        "def generate_query(question):\n",
        "    updated_prompt = prompt.format(question=question)\n",
        "    inputs = tokenizer(updated_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        max_new_tokens=400,\n",
        "        do_sample=False,\n",
        "        num_beams=4,\n",
        "    )\n",
        "    outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "    # empty cache so that you do generate more results w/o memory crashing\n",
        "    # particularly important on Colab – memory management is much more straightforward\n",
        "    # when running on an inference service\n",
        "    return sqlparse.format(outputs[0].split(\"[SQL]\")[-1], reindent=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCJvcurqlP2_"
      },
      "outputs": [],
      "source": [
        "question = \"What was our revenue by product in the New York region last month?\"\n",
        "generated_sql = generate_query(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flEJuo1OpYQ8",
        "outputId": "6afa2baa-570b-4e31-98b3-3532d603307a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "SELECT p.product_id,\n",
            "       SUM(s.quantity * p.price) AS revenue\n",
            "FROM sales s\n",
            "JOIN salespeople sp ON s.salesperson_id = sp.salesperson_id\n",
            "JOIN products p ON s.product_id = p.product_id\n",
            "WHERE sp.region = 'New York'\n",
            "  AND s.sale_date >= (CURRENT_DATE - INTERVAL '1 month')\n",
            "GROUP BY p.product_id\n",
            "ORDER BY revenue DESC NULLS LAST;\n"
          ]
        }
      ],
      "source": [
        "print(generated_sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09ysYJbsnKUh"
      },
      "source": [
        "# Exercise\n",
        " - Complete the prompts similar to what we did in class.\n",
        "     - Try at least 3 versions\n",
        "     - Be creative\n",
        " - Write a one page report summarizing your findings.\n",
        "     - Were there variations that didn't work well? i.e., where GPT either hallucinated or wrong\n",
        " - What did you learn?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRpnSvwLnuuY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}